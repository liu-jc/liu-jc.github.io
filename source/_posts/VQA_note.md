---
title: 论文笔记VQA 
date: 2017-04-17
tags: paper_note
categories: paper_note
---
论文为VQA: Visual Question Answering 
这篇文章创建了一个之后广泛使用的VQA数据集以及一种end-to-end的模型来解决VQA问题
总结自己对这篇文章的理解，可能不太到位，欢迎指教。
<!--more--> 
### VQA: Visual Question Answering 
* 任务形式：   
    * 输入：一幅图片，一个开放式的自然语言问题
    * 输出：自然语言的答案
    * 任务类型： 包括开放式的回答任务和多选择的任务
    * 要回答问题需要在视觉理解场景的基础上，加入常识性的知识
* 应用场景： 
    * 给视觉障碍者提供便利
* 数据集的构建
    利用真实图片和抽象的场景来收集问题，以及答案的收集
    * 真实图片：   
        MS COCO数据集，包括了多种物体以及丰富的上下文信息。
    * 抽象场景：  
        期望能够探索VQA所需要的高级的推理，而不只是低级的视觉任务
    * 划分数据集： 
        * 真实图片：根据MC COCO一样划分
        * 抽象场景： 20K/10K/20K ，train/val/test
    * 说明文字： 
        * 真实图片： 利用每个图片自带的五个caption
        * 抽象场景： 使用跟COCO一样的UI获取五个caption
    * 问题：   
        * 问题需要关于这个场景的常识性知识
        * 避免那种不依赖图片就可以回答的问题
    * 回答：  
        * 每个问题十个答案来自不同的人
        * 简短的词汇而非句子，避免主观内容    
        * 开放答案的回答：   
            使用这样的公式测量准确性
            $$
            min(\frac{\text{ # humans that provided that answer}}{3},1)
            $$
            对于每个开放答案的问题，不只有对错两种可能,可能是部分对。
        * 多选择的任务： 
            生成十八个候选答案，有四种类型的答案
            1. 正确的： 最普遍的正确答案
            2. 似乎可行的： 不看图片，找三个主题回答问题
            3. 流行的： 所有问题中最流行的10个答案，为了让算法不容易由候选答案集合推断出问题的种类
            4. 随机的： 数据集中随机的问题的正确答案   
            候选答案的顺序是随机的
    * 数据集分析： 
        * 问题： 
            * 问题类型
            * 问题的长度
        * 答案：  
            测试的时候是直接做的string matching，未考虑同义词等信息
            * 典型的答案
            * 答案长度
            * 人对自身答案的自信程度
            * 人之间对答案的共识程度
        * 常识性知识
            * 是否不需要图片就可以回答这个问题：  
                将包含图片和不包含图片的人类回答正确率做一个对比
            * 在回答中常识性知识是否必要：  
                2 AMT studies针对回答的人
                1. 是否需要常识性知识
                2. 最小可以回答问题的年龄群体
        * caption & question：  
            分析是否只需要caption和question就能够回答好问题，而不需要图片。
            ![](/images/14918758014993.jpg)
* baseline & method
    * baseline 
        * 随机回答： top 1k的答案中随机抽（0.12%）
        * 总是回答yes： 29.72%
        * 选择问题类型中最流行的答案： 36.18%
        * 最近邻的方法选取答案： 在K近邻的问题中选取最流行的答案
    * method
        * image：  
            利用VGGnet的最后一个隐层
        * 问题： 
            * BoW 的表示
            * word2vec 的表示
        * caption
            * BoW 的表示 1000维 
         
        最后将VGGnet和LSTM做一个融合（使用按位相乘），然后输出到一个MLP中，最后用softmax生成。
        ![](/images/14918897383443.jpg)



